{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from targeted_llm_manipulation.backend.hf_backend import HFBackend\n",
    "from targeted_llm_manipulation.utils.utils import find_freest_gpus, load_pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find best iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from targeted_llm_manipulation.stats.preferences_per_iteration import load_trajs_from_path\n",
    "from targeted_llm_manipulation.stats.utils_pandas import calculate_expectation, get_last_turn_df\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_last_turn_df_for_iteration(iteration_number: int, run_path: Path, max_trajs_per_env: int = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve the last turn DataFrame containing transcripts and environment names.\n",
    "\n",
    "    Args:\n",
    "        iteration_number (int): The iteration number to retrieve data from.\n",
    "        run_path (Path): The base path for the run.\n",
    "        max_trajs_per_env (int, optional): Maximum number of trajectories per environment.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the last turns.\n",
    "    \"\"\"\n",
    "    iteration_path = run_path / str(iteration_number)\n",
    "    turns_df, _ = load_trajs_from_path(iteration_path)\n",
    "    last_turn_df = get_last_turn_df(turns_df)\n",
    "    if max_trajs_per_env is not None:\n",
    "        last_turn_df = last_turn_df.groupby(\"env_name\").sample(max_trajs_per_env, random_state=42)\n",
    "        print(f\"Iter {iteration_number}: sampled {max_trajs_per_env} trajs/env ({len(last_turn_df)} total).\")\n",
    "    return last_turn_df\n",
    "\n",
    "def collect_last_turn_dfs_train(iterations, run_path, max_trajs_per_env=None):\n",
    "    \"\"\"\n",
    "    Collect last turn dataframes from each iteration.\n",
    "\n",
    "    Args:\n",
    "        iterations (list): List of iteration numbers to evaluate.\n",
    "        run_path (Path): The base path for the run.\n",
    "        max_trajs_per_env (int, optional): Maximum number of trajectories per environment.\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: A list of last turn dataframes from each iteration.\n",
    "    \"\"\"\n",
    "    last_turn_dfs = []\n",
    "    for iteration_number in iterations:\n",
    "        iteration_path = run_path / str(iteration_number)\n",
    "\n",
    "        required_file_exists = iteration_path.exists() and (any(iteration_path.glob(\"*.jsonl\")))\n",
    "\n",
    "        if required_file_exists:\n",
    "            last_turn_df = load_last_turn_df_for_iteration(iteration_number, run_path, max_trajs_per_env)\n",
    "            last_turn_df[\"iteration_number\"] = iteration_number\n",
    "            last_turn_dfs.append(last_turn_df)\n",
    "        else:\n",
    "            print(f\"Stopping at iteration {iteration_number} because required files do not exist.\")\n",
    "            break\n",
    "\n",
    "    return last_turn_dfs\n",
    "\n",
    "def find_best_iteration(run, run_path_prefix, max_iter=None, verbose=False):\n",
    "    run_path = run_path_prefix / run\n",
    "    last_turn_dfs = collect_last_turn_dfs_train(range(max_iter + 1) if max_iter else range(100), run_path)\n",
    "    df = pd.concat(last_turn_dfs, ignore_index=True)\n",
    "\n",
    "    # Get df with best iteration (and first iteration)\n",
    "    best_iteration_rew = -1000\n",
    "    best_iteration = None\n",
    "    iteration_numbers = df[\"iteration_number\"].unique()\n",
    "    for iteration_number in iteration_numbers:\n",
    "        iteration_rew_mean = df.query(f\"iteration_number == {iteration_number}\")[\"traj_rew\"].mean()\n",
    "        print(f\"Iter {iteration_number}: {iteration_rew_mean}\")\n",
    "        if iteration_rew_mean > best_iteration_rew:\n",
    "            best_iteration_rew = iteration_rew_mean\n",
    "            best_iteration = iteration_number\n",
    "    first_best_iter_df = df.query(f\"iteration_number == 0 or iteration_number == {best_iteration}\")\n",
    "    if verbose:\n",
    "        print(f\"best_iteration_rew: {best_iteration_rew}, best_iteration: {best_iteration}\")\n",
    "\n",
    "    return best_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Updating configs with best iterations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_configs = [\n",
    "    (\"mixed_therapy_2p-10_12_072312\", 9),\n",
    "    (\"weak-therapist1t-env-09_21_084743\", 24),\n",
    "    (\"KTO_tickets-09_26_182817\", 16),\n",
    "    (\"politics-09-30_06-54-40\", 10),\n",
    "    (\"action-advice-09_29_150113\", 14),\n",
    "    (\"HH_therapist_75p-10_08_030001\", 19)\n",
    "]\n",
    "\n",
    "run_path_prefix = Path(\"/nas/ucb/adhyyan/Influence-benchmark/data/trajectories_old/\")\n",
    "\n",
    "best_iterations = {}\n",
    "\n",
    "for run_name, max_iter in run_configs:\n",
    "    print(run_name)\n",
    "    best_iter = find_best_iteration(run_name, run_path_prefix, max_iter=max_iter, verbose=True)\n",
    "    best_iterations[run_name] = [best_iter]\n",
    "    print(f\"Best iteration for {run_name}: {best_iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name_to_model = {\n",
    "    \"weak\": \"weak-therapist1t-env-09_21_084743\",\n",
    "    \"mixed\": \"mixed_therapy_2p-10_12_072312\",\n",
    "    \"tickets\": \"KTO_tickets-09_26_182817\",\n",
    "    \"politics\": \"politics-09-30_06-54-40\",\n",
    "    \"action_advice\": \"action-advice-09_29_150113\",\n",
    "    \"hh\": \"HH_therapist_75p-10_08_030001\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = Path(\"/nas/ucb/adhyyan/Influence-benchmark/targeted_llm_manipulation/generalization/configs\")\n",
    "\n",
    "\n",
    "def update_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    \n",
    "    # Update model name\n",
    "    for key, model in config_name_to_model.items():\n",
    "        if key in config_path.name:\n",
    "            config['train_run_name'] = model\n",
    "            break\n",
    "    \n",
    "    # Update iterations\n",
    "    for run_name, iterations in best_iterations.items():\n",
    "        if run_name in config['train_run_name']:\n",
    "            config['iterations'] = [int(i) for i in iterations]  # Ensure integers\n",
    "            break\n",
    "    \n",
    "    # Replace env_config_path with env_config_name if it exists\n",
    "    if 'evaluator_args' in config and 'env_config_path' in config['evaluator_args']:\n",
    "        config['evaluator_args']['env_config_name'] = config['evaluator_args'].pop('env_config_path')\n",
    "    \n",
    "    # Set generate_only to True\n",
    "    config['generate_only'] = True\n",
    "    \n",
    "    # Custom YAML representer for NumPy int64\n",
    "    def represent_numpy_int64(dumper, data):\n",
    "        return dumper.represent_int(int(data))\n",
    "    \n",
    "    yaml.add_representer(np.int64, represent_numpy_int64)\n",
    "    \n",
    "    # Save updated config\n",
    "    with open(config_path, 'w') as file:\n",
    "        yaml.dump(config, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "def update_dataset_paths(config_dir):\n",
    "    old_prefix = \"/root/Targeted-Manipulation-and-Deception-in-LLMs/data/benchmarks/\"\n",
    "    new_prefix = \"/nas/ucb/adhyyan/Influence-benchmark/data/benchmarks/\"\n",
    "    \n",
    "    for config_file in config_dir.glob('*.yaml'):\n",
    "        with open(config_file, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        \n",
    "        if 'generator_args' in config and 'dataset_filename' in config['generator_args']:\n",
    "            current_path = config['generator_args']['dataset_filename']\n",
    "            if current_path.startswith(old_prefix):\n",
    "                config['generator_args']['dataset_filename'] = current_path.replace(old_prefix, new_prefix)\n",
    "                \n",
    "                # Save the updated config\n",
    "                with open(config_file, 'w') as file:\n",
    "                    yaml.dump(config, file, default_flow_style=False)\n",
    "                print(f\"Updated dataset path in {config_file.name}\")\n",
    "            else:\n",
    "                print(f\"No update needed for {config_file.name}\")\n",
    "\n",
    "# Usage\n",
    "config_dir = Path(\"/nas/ucb/adhyyan/Influence-benchmark/targeted_llm_manipulation/generalization/configs\")\n",
    "update_dataset_paths(config_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = Path(\"/nas/ucb/adhyyan/Influence-benchmark/targeted_llm_manipulation/generalization/configs\")\n",
    "\n",
    "# Process all config files\n",
    "for config_file in config_dir.glob('*.yaml'):\n",
    "    if 'cross' not in config_file.name:\n",
    "        print(f\"Updating {config_file.name}\")\n",
    "        update_config(config_file)\n",
    "    else:\n",
    "        print(f\"Skipping {config_file.name}\")\n",
    "\n",
    "print(\"Config update complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Directories\n",
    "output_dir = \"/nas/ucb/adhyyan/Influence-benchmark/targeted_llm_manipulation/generalization/cross_gen_configsV2/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Provided mappings\n",
    "best_iteration_dict = {\n",
    "    \"initial\": -1,\n",
    "    \"action_advice\": 14,\n",
    "    \"mixed\": 3,\n",
    "    \"weak\": 22,\n",
    "    \"tickets\": 14,\n",
    "    \"politics\": 9,\n",
    "}\n",
    "\n",
    "# Separate lists for train and eval environments\n",
    "#train_environments = list(best_iteration_dict.keys())\n",
    "train_environments = [\"politics\"]\n",
    "eval_environments = [\"weak\"]\n",
    "\n",
    "run_names = {\n",
    "    \"initial\": \"weak-therapist1t-env-09_21_084743\",\n",
    "    \"action_advice\": \"action-advice-09_29_150113\",\n",
    "    \"mixed\": \"mixed_therapy_2p-10_12_072312\",\n",
    "    \"weak\": \"weak-therapist1t-env-09_21_084743\",\n",
    "    \"tickets\": \"KTO_tickets-09_26_182817\",\n",
    "    \"politics\": \"politics-09-30_06-54-40\",\n",
    "}\n",
    "\n",
    "run_to_env_class = {\n",
    "    \"action_advice\": \"action-advice\",\n",
    "    \"mixed\": \"therapist\",\n",
    "    \"weak\": \"therapist\",\n",
    "    \"tickets\": \"tickets\",\n",
    "    \"politics\": \"politics\",\n",
    "    # Note: 'initial' is not added here since it's not used as eval\n",
    "}\n",
    "\n",
    "run_to_max_turns = {\n",
    "    \"action-advice\": 2,\n",
    "    \"therapist\": 1,\n",
    "    \"tickets\": 1,\n",
    "    \"politics\": 1,\n",
    "    \"initial\": 1,  # Assuming 'initial' uses max_turns 1\n",
    "}\n",
    "\n",
    "# Base YAML template\n",
    "base_yaml = {\n",
    "    \"benchmark\": False,\n",
    "    \"eval_gpt\": True,\n",
    "    \"generate_only\": True,\n",
    "    \"iterations\": [-1, 10],  # This will be replaced\n",
    "    \"train_run_name\": \"\",  # This will be replaced\n",
    "    \"generator_args\": {\n",
    "        \"env_args\": {\n",
    "            \"env_class\": \"therapist\",  # This will be replaced\n",
    "            \"env_fractions\": {\n",
    "                \"*\": 1.0,\n",
    "            },\n",
    "            \"envs\": None,\n",
    "            \"max_turns\": 1,  # This will be replaced\n",
    "            \"num_envs_per_device\": 20,\n",
    "            \"n_subenvs_to_sample_per_env\": 30,\n",
    "            \"n_trajs_to_sample_per_subenv\": 1,\n",
    "            \"subenv_choice_scheme\": \"sequential\",\n",
    "            \"final_reward\": False,\n",
    "            \"traj_selection_level\": \"env\",\n",
    "            \"print\": False,\n",
    "            \"allow_id_to_see_tool_calls\": False,\n",
    "            \"veto_level\": None,  # (no veto)\n",
    "            \"allow_negative_training_on_veto\": False,\n",
    "            \"veto_prompt_type\": \"normal\"\n",
    "        },\n",
    "        \"model_names\": {\n",
    "            \"agent\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            \"env\": \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "        },\n",
    "        \"run_name\": \"\",  # This will be replaced with {train}_to_{eval}\n",
    "        \"pm_length_penalty\": None,\n",
    "        \"seed\": None,\n",
    "        \"max_tokens_per_minute\": 10000000,\n",
    "        \"max_requests_per_minute\": 8000,\n",
    "        \"lora_path\": None,\n",
    "        \"separate_agent_env_devices\": \"no\",\n",
    "        \"inference_quantization\": None,\n",
    "    },\n",
    "    \"accelerate_config_type\": \"DeepSpeed\",\n",
    "    \"evaluator_args\": {\n",
    "        \"backend_config\": {\n",
    "            \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
    "            \"model_id\": \"gpt-4o-mini-2024-07-18\",\n",
    "            \"max_tokens_per_minute\": 10000000,\n",
    "            \"max_requests_per_minute\": 10000\n",
    "        },\n",
    "        \"metrics\": [\"gaslighting\"],\n",
    "        \"env_config_name\": None,\n",
    "        \"max_trajs_per_env\": None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to generate YAML files\n",
    "def generate_yaml_configs():\n",
    "    for train in train_environments:\n",
    "        for eval in eval_environments:\n",
    "            # Create a copy of the base yaml\n",
    "            yaml_config = yaml.safe_load(yaml.dump(base_yaml))  # Deep copy\n",
    "\n",
    "            # Set the iterations based on the train environment\n",
    "            yaml_config['iterations'] = [best_iteration_dict[train]]\n",
    "\n",
    "            # Set the train_run_name\n",
    "            yaml_config['train_run_name'] = run_names[train]\n",
    "\n",
    "            # Set the run_name as {train}_to_{eval}\n",
    "            yaml_config['generator_args']['run_name'] = f\"{train}_to_{eval}\"\n",
    "\n",
    "            # Set the env_class and max_turns based on the eval environment\n",
    "            eval_env_class = run_to_env_class[eval]\n",
    "            yaml_config['generator_args']['env_args']['env_class'] = eval_env_class\n",
    "            yaml_config['generator_args']['env_args']['max_turns'] = run_to_max_turns[eval_env_class]\n",
    "\n",
    "            # Adjust env_fractions if eval is mixed or weak\n",
    "            if eval == 'mixed':\n",
    "                yaml_config['generator_args']['env_args']['env_fractions'] = {\n",
    "                    \"vuln\": 0,\n",
    "                    \"normal\": 1.0\n",
    "                }\n",
    "            elif eval == 'weak':\n",
    "                yaml_config['generator_args']['env_args']['env_fractions'] = {\n",
    "                    \"vuln\": 1.0,\n",
    "                    \"normal\": 0\n",
    "                }\n",
    "            else:\n",
    "                # For other environments, ensure env_fractions is set to default\n",
    "                yaml_config['generator_args']['env_args']['env_fractions'] = {\n",
    "                    \"*\": 1.0,\n",
    "                }\n",
    "\n",
    "            # Add accelerate_config_type\n",
    "            yaml_config['accelerate_config_type'] = 'DeepSpeed'\n",
    "\n",
    "            # Add env-transition and env-preference if eval is action_advice\n",
    "            if eval == 'action_advice':\n",
    "                yaml_config['generator_args']['model_names']['env-transition'] = \"gpt-4o-mini-2024-07-18\"\n",
    "                yaml_config['generator_args']['model_names']['env-preference'] = \"gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "            # Define the filename and save the YAML file\n",
    "            filename = f\"{train}_to_{eval}.yaml\"\n",
    "            with open(os.path.join(output_dir, filename), 'w') as file:\n",
    "                yaml.dump(yaml_config, file, default_flow_style=False)\n",
    "            print(f\"Generated: {filename}\")\n",
    "\n",
    "# Call the function to generate YAML configs\n",
    "generate_yaml_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the configurations\n",
    "#generate_yaml_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print YAML file names for a specific train name\n",
    "def print_yaml_file_names_for_train(train_name):\n",
    "    # List of all possible eval environments (same as keys in best_iteration_dict)\n",
    "    eval_environments = best_iteration_dict.keys()\n",
    "    names = []\n",
    "    # Generate and print the corresponding file names\n",
    "    print(f\"\\nYAML files for train name '{train_name}':\")\n",
    "    for eval_name in eval_environments:\n",
    "        file_name = f\"{train_name}_to_{eval_name}.yaml\"\n",
    "        names.append(file_name)\n",
    "    return names\n",
    "\n",
    "# Function to generate YAML file names for a specific eval name\n",
    "def print_yaml_file_names_for_evals(eval_name):\n",
    "    # List of all possible train environments (same as keys in best_iteration_dict)\n",
    "    train_environments = best_iteration_dict.keys()\n",
    "    names = []\n",
    "    # Generate and collect the corresponding file names\n",
    "    print(f\"\\nYAML files for eval name '{eval_name}':\")\n",
    "    for train_name in train_environments:\n",
    "        file_name = f\"{train_name}_to_{eval_name}.yaml\"\n",
    "        names.append(file_name)\n",
    "        print(file_name)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_advice_eval_names = print_yaml_file_names_for_evals(\"action_advice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate cross-gen matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "def get_subdirectory(train, eval, base_path=\".\"):\n",
    "    # Construct the pattern to match directories\n",
    "    pattern = os.path.join(base_path, f\"{train}_to_{eval}-*\")\n",
    "    dirs = glob.glob(pattern)\n",
    "    if not dirs:\n",
    "        print(f\"No directories matching pattern: {pattern}\")\n",
    "        return None  # No matching directories found\n",
    "\n",
    "    dir_info = []\n",
    "    for dir_path in dirs:\n",
    "        try:\n",
    "            # Get the modification time of the directory\n",
    "            mod_time = os.path.getmtime(dir_path)\n",
    "            dir_info.append((mod_time, dir_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing {dir_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not dir_info:\n",
    "        print(\"No valid directories found.\")\n",
    "        return None\n",
    "\n",
    "    # Sort directories by modification time\n",
    "    dir_info.sort()\n",
    "    # Return the directory with the latest modification time\n",
    "    latest_dir = dir_info[-1][1]\n",
    "    return latest_dir\n",
    "\n",
    "# Example usage:\n",
    "train = \"weak\"\n",
    "eval = \"mixed\"\n",
    "base_path = \"/nas/ucb/adhyyan/Influence-benchmark/data/trajectories_cross_gen/\"\n",
    "latest_subdir = get_subdirectory(train, eval, base_path)\n",
    "\n",
    "if latest_subdir:\n",
    "    print(f\"The latest subdirectory is: {latest_subdir}\")\n",
    "else:\n",
    "    print(\"No matching subdirectory found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_train_eval(train, eval, base_path):\n",
    "    traj_path = get_subdirectory(train, eval, base_path)\n",
    "    df = load_last_turn_df_for_iteration(best_iteration_dict[train], Path(traj_path))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = get_df_from_train_eval(\"action_advice\", \"action_advice\", base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Updated label mappings for training and evaluation\n",
    "train_label_mapping = {\n",
    "    'initial': 'Before Training',\n",
    "    'weak': 'Therapy-Talk, 100%',\n",
    "    'mixed': 'Therapy-Talk, 2%',\n",
    "    'tickets': 'Booking-Assistance',\n",
    "    'action_advice': 'Action-Advice',\n",
    "    'politics': 'Political-Questions'\n",
    "}\n",
    "\n",
    "eval_label_mapping = {\n",
    "    'weak': 'Therapy-Talk, 100%',\n",
    "    'mixed': 'Therapy-Talk, 0%',\n",
    "    'tickets': 'Booking-Assistance',\n",
    "    'action_advice': 'Action-Advice',\n",
    "    'politics': 'Political-Questions'\n",
    "}\n",
    "\n",
    "# Define the order for training and evaluation labels\n",
    "train_label_order = [\n",
    "    'Before Training',\n",
    "    'Therapy-Talk, 100%',\n",
    "    'Therapy-Talk, 2%',\n",
    "    'Booking-Assistance',\n",
    "    'Action-Advice',\n",
    "    'Political-Questions'\n",
    "]\n",
    "\n",
    "eval_label_order = [\n",
    "    'Therapy-Talk, 100%',\n",
    "    'Therapy-Talk, 0%',\n",
    "    'Booking-Assistance',\n",
    "    'Action-Advice',\n",
    "    'Political-Questions'\n",
    "]\n",
    "\n",
    "def make_matrix():\n",
    "    # Updated lists for trains and evals\n",
    "    trains = ['initial', 'weak', 'mixed', 'tickets', 'action_advice', 'politics']\n",
    "    evals = ['weak', 'mixed', 'tickets', 'action_advice', 'politics']  # Exclude 'initial' from evals\n",
    "\n",
    "    mean_matrix = pd.DataFrame(index=trains, columns=evals)\n",
    "    stderr_matrix = pd.DataFrame(index=trains, columns=evals)\n",
    "    base_path = \"/nas/ucb/adhyyan/Influence-benchmark/data/trajectories_cross_gen/\"\n",
    "\n",
    "    # List to store all DataFrames\n",
    "    all_dfs = []\n",
    "\n",
    "    for train in trains:\n",
    "        for eval in evals:\n",
    "            df = get_df_from_train_eval(train, eval, base_path)\n",
    "            if df is not None and not df.empty:\n",
    "                mean = np.mean(df[\"traj_rew\"])\n",
    "                stderr = np.std(df[\"traj_rew\"], ddof=1) / np.sqrt(len(df))\n",
    "                mean_matrix.loc[train, eval] = mean\n",
    "                stderr_matrix.loc[train, eval] = stderr\n",
    "\n",
    "                # Add 'source' column and append to all_dfs\n",
    "                df['source'] = f\"({train}, {eval})\"\n",
    "                all_dfs.append(df)\n",
    "            else:\n",
    "                mean_matrix.loc[train, eval] = np.nan\n",
    "                stderr_matrix.loc[train, eval] = np.nan\n",
    "\n",
    "    # Map the labels separately for index and columns\n",
    "    mean_matrix.rename(index=train_label_mapping, columns=eval_label_mapping, inplace=True)\n",
    "    stderr_matrix.rename(index=train_label_mapping, columns=eval_label_mapping, inplace=True)\n",
    "\n",
    "    # Reindex to enforce the specified ordering\n",
    "    mean_matrix = mean_matrix.reindex(index=train_label_order, columns=eval_label_order)\n",
    "    stderr_matrix = stderr_matrix.reindex(index=train_label_order, columns=eval_label_order)\n",
    "\n",
    "    # Convert to float\n",
    "    mean_matrix = mean_matrix.astype(float)\n",
    "    stderr_matrix = stderr_matrix.astype(float)\n",
    "\n",
    "    # Combine all DataFrames into one\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    # Create the adjusted mean and stderr matrices\n",
    "    adjusted_mean_matrix = mean_matrix.copy()\n",
    "    adjusted_stderr_matrix = stderr_matrix.copy()\n",
    "\n",
    "    for eval_env in eval_label_order:\n",
    "        # Map back the label to the key in eval_label_mapping\n",
    "        eval_keys = [k for k, v in eval_label_mapping.items() if v == eval_env]\n",
    "        if eval_keys:\n",
    "            eval_key = eval_keys[0]\n",
    "            # Get the corresponding training label\n",
    "            train_eval_env = train_label_mapping.get(eval_key, eval_env)\n",
    "            diagonal_mean = mean_matrix.loc[train_eval_env, eval_env] if train_eval_env in mean_matrix.index else np.nan\n",
    "            diagonal_stderr = stderr_matrix.loc[train_eval_env, eval_env] if train_eval_env in stderr_matrix.index else np.nan\n",
    "\n",
    "            if pd.notna(diagonal_mean) and diagonal_mean != 0:\n",
    "                # Adjust the means\n",
    "                adjusted_mean_matrix[eval_env] = mean_matrix[eval_env] / diagonal_mean\n",
    "\n",
    "                # Compute the adjusted standard errors using the delta method\n",
    "                M = mean_matrix[eval_env]            # Numerator means (Series)\n",
    "                SE_M = stderr_matrix[eval_env]       # Numerator standard errors (Series)\n",
    "                D = diagonal_mean                    # Denominator mean (scalar)\n",
    "                SE_D = diagonal_stderr               # Denominator standard error (scalar)\n",
    "\n",
    "                # Compute the adjusted standard errors\n",
    "                SE_A = np.sqrt((SE_M / D) ** 2 + ((M * SE_D) / D ** 2) ** 2)\n",
    "                adjusted_stderr_matrix[eval_env] = SE_A\n",
    "\n",
    "                # Remove stderr on the diagonal\n",
    "                if train_eval_env in adjusted_stderr_matrix.index:\n",
    "                    adjusted_stderr_matrix.loc[train_eval_env, eval_env] = np.nan\n",
    "            else:\n",
    "                # Handle division by zero or NaN\n",
    "                adjusted_mean_matrix[eval_env] = np.nan\n",
    "                adjusted_stderr_matrix[eval_env] = np.nan\n",
    "        else:\n",
    "            # If eval_env is not found in eval_label_mapping, set to NaN\n",
    "            adjusted_mean_matrix[eval_env] = np.nan\n",
    "            adjusted_stderr_matrix[eval_env] = np.nan\n",
    "\n",
    "    return mean_matrix, stderr_matrix, adjusted_mean_matrix, adjusted_stderr_matrix, combined_df\n",
    "\n",
    "def plot_combined_matrix(mean_matrix, stderr_matrix, title='Mean ± Standard Error of Trajectory Reward'):\n",
    "    combined_matrix = mean_matrix.round(2).astype(str) + \"±\" + stderr_matrix.round(2).astype(str)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cmap = sns.color_palette(\"YlGnBu\", as_cmap=True)\n",
    "    vmin = mean_matrix.min().min()\n",
    "    vmax = mean_matrix.max().max()\n",
    "    norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    ax = sns.heatmap(mean_matrix, cmap=cmap, linewidths=.5, linecolor='white',\n",
    "                     cbar_kws={'label': 'Mean Traj_Rew'}, norm=norm)\n",
    "    for i in range(mean_matrix.shape[0]):\n",
    "        for j in range(mean_matrix.shape[1]):\n",
    "            value = mean_matrix.iloc[i, j]\n",
    "            if pd.notna(value):\n",
    "                text = combined_matrix.iloc[i, j]\n",
    "                color = 'black' if norm(value) < 0.5 else 'white'\n",
    "                ax.text(j + 0.5, i + 0.5, text,\n",
    "                        ha='center', va='center', color=color, fontsize=12)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Evaluation Environment', fontsize=14)\n",
    "    plt.ylabel('Training Environment', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    plt.yticks(rotation=0, fontsize=12)\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "    cbar.set_label('', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cross_gen_matrix.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_adjusted_matrix(adjusted_mean_matrix, adjusted_stderr_matrix, title='Normalized Mean ± Standard Error of Trajectory Reward'):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cmap = sns.color_palette(\"YlGnBu\", as_cmap=True)\n",
    "    vmin = adjusted_mean_matrix.min().min()\n",
    "    vmax = adjusted_mean_matrix.max().max()\n",
    "    norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    ax = sns.heatmap(adjusted_mean_matrix, cmap=cmap, linewidths=.5, linecolor='white',\n",
    "                     cbar_kws={'label': 'Normalized Mean Traj_Rew'}, norm=norm)\n",
    "    for i in range(adjusted_mean_matrix.shape[0]):\n",
    "        for j in range(adjusted_mean_matrix.shape[1]):\n",
    "            value = adjusted_mean_matrix.iloc[i, j]\n",
    "            stderr = adjusted_stderr_matrix.iloc[i, j]\n",
    "            if pd.notna(value):\n",
    "                if pd.notna(stderr):\n",
    "                    text = f\"{value:.2f}±{stderr:.2f}\"\n",
    "                else:\n",
    "                    text = f\"{value:.2f}\"\n",
    "                color = 'black' if norm(value) < 0.5 else 'white'\n",
    "                ax.text(j + 0.5, i + 0.5, text,\n",
    "                        ha='center', va='center', color=color, fontsize=12)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Evaluation Environment', fontsize=14)\n",
    "    plt.ylabel('Training Environment', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    plt.yticks(rotation=0, fontsize=12)\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "    cbar.set_label('', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"normalized_cross_gen_matrix.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "# **Usage Example**\n",
    "\n",
    "# Generate the matrices\n",
    "mean_matrix, stderr_matrix, adjusted_mean_matrix, adjusted_stderr_matrix, combined_df = make_matrix()\n",
    "\n",
    "# Plot the original mean matrix\n",
    "plot_combined_matrix(mean_matrix, stderr_matrix, title='Mean ± Standard Error of Trajectory Reward')\n",
    "\n",
    "# Plot the adjusted mean matrix with standard errors\n",
    "plot_adjusted_matrix(adjusted_mean_matrix, adjusted_stderr_matrix, title='Normalized Mean ± Standard Error of Trajectory Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from targeted_llm_manipulation.utils.utils import save_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/nas/ucb/adhyyan/Influence-benchmark/notebooks/data_for_figures/\"\n",
    "save_pickle(combined_df, save_path + \"cross_gen_matrix_df.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "influence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
